"""
ML Training Pipeline - Main Training Module
Complete end-to-end training pipeline.

Pipeline:
1. Load datasets (CIC-IDS 2017, UNSW-NB15, CTU-13)
2. Extract features
3. Apply class balancing (SMOTE/undersampling)
4. Train per-attack classifiers
5. Train ensemble model
6. Validate and export
"""

import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import json

# Local imports
from .dataset_loaders import UnifiedDatasetLoader, load_dataset
from .feature_extraction import FeatureExtractor
from .attack_classifiers import AttackCategoryManager, create_attack_classifier

# sklearn imports
try:
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score,
        confusion_matrix, classification_report, roc_auc_score
    )
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ sklearn not available. Limited evaluation metrics.")

# SMOTE for class balancing
try:
    from imblearn.over_sampling import SMOTE
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.pipeline import Pipeline as ImbPipeline
    IMBLEARN_AVAILABLE = True
except ImportError:
    IMBLEARN_AVAILABLE = False
    print("âš ï¸ imbalanced-learn not available. No SMOTE oversampling.")


class TrainingPipeline:
    """
    Complete ML training pipeline for PCDS detection engine
    
    Features:
    - Multi-dataset loading
    - Class balancing (SMOTE + undersampling)
    - Per-attack-category training
    - Ensemble model training
    - Comprehensive validation
    - Model export
    """
    
    def __init__(
        self,
        data_dir: str = "ml/datasets",
        model_dir: str = "ml/models/trained",
        feature_dim: int = 64
    ):
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        self.feature_dim = feature_dim
        self.dataset_loader = UnifiedDatasetLoader(data_dir)
        self.feature_extractor = FeatureExtractor(feature_dim)
        self.category_manager = AttackCategoryManager(feature_dim, model_dir)
        
        # Training stats
        self.training_history = {
            'started_at': None,
            'completed_at': None,
            'datasets_loaded': [],
            'samples_total': 0,
            'samples_per_category': {},
            'metrics': {}
        }
        
        print("âœ… Training Pipeline initialized")
    
    def load_data(self, datasets: List[str] = None) -> Dict[str, Tuple[np.ndarray, np.ndarray, np.ndarray]]:
        """
        Load training datasets
        
        Args:
            datasets: List of dataset names to load. If None, loads all available.
        
        Returns:
            Dict mapping dataset name to (X, y_binary, y_category) tuples
        """
        print("\nğŸ“¥ Loading datasets...")
        
        loaded = {}
        
        if datasets:
            for name in datasets:
                try:
                    X, y_bin, y_cat = load_dataset(name)
                    loaded[name] = (X, y_bin, y_cat)
                    self.training_history['datasets_loaded'].append(name)
                except FileNotFoundError as e:
                    print(f"   âš ï¸ {name}: {e}")
        else:
            loaded = self.dataset_loader.load_all()
            self.training_history['datasets_loaded'] = list(loaded.keys())
        
        # Calculate totals
        total_samples = sum(len(X) for X, _, _ in loaded.values())
        self.training_history['samples_total'] = total_samples
        
        print(f"âœ… Loaded {len(loaded)} datasets, {total_samples} total samples")
        
        return loaded
    
    def apply_class_balancing(
        self,
        X: np.ndarray,
        y: np.ndarray,
        strategy: str = 'combined'
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Apply class balancing to handle imbalanced datasets
        
        Args:
            X: Feature matrix
            y: Labels
            strategy: 'oversample' (SMOTE), 'undersample', or 'combined'
        
        Returns:
            Balanced (X, y)
        """
        if not IMBLEARN_AVAILABLE:
            print("   âš ï¸ imbalanced-learn not available, skipping balancing")
            return X, y
        
        print(f"   âš–ï¸ Applying {strategy} balancing...")
        print(f"      Before: {np.bincount(y.astype(int))}")
        
        if strategy == 'oversample':
            sampler = SMOTE(random_state=42)
        elif strategy == 'undersample':
            sampler = RandomUnderSampler(random_state=42)
        else:  # combined
            oversample = SMOTE(sampling_strategy=0.5, random_state=42)
            undersample = RandomUnderSampler(sampling_strategy=0.8, random_state=42)
            sampler = ImbPipeline([
                ('over', oversample),
                ('under', undersample)
            ])
        
        try:
            X_balanced, y_balanced = sampler.fit_resample(X, y)
            print(f"      After: {np.bincount(y_balanced.astype(int))}")
            return X_balanced, y_balanced
        except Exception as e:
            print(f"      âš ï¸ Balancing failed: {e}")
            return X, y
    
    def create_splits(
        self,
        X: np.ndarray,
        y: np.ndarray,
        test_size: float = 0.2,
        val_size: float = 0.2
    ) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
        """Create train/validation/test splits (60/20/20)"""
        
        # First split: 80/20 for train+val/test
        X_trainval, X_test, y_trainval, y_test = train_test_split(
            X, y, test_size=test_size, stratify=y, random_state=42
        )
        
        # Second split: adjust for val
        val_adjusted = val_size / (1 - test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_trainval, y_trainval, test_size=val_adjusted, stratify=y_trainval, random_state=42
        )
        
        print(f"   ğŸ“Š Split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")
        
        return {
            'train': (X_train, y_train),
            'val': (X_val, y_val),
            'test': (X_test, y_test)
        }
    
    def train_per_category(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        epochs: int = 50
    ) -> Dict[str, Dict]:
        """
        Train individual classifiers for each attack category
        
        Args:
            X_train, y_train: Training data (y should be category labels)
            X_val, y_val: Validation data
            epochs: Training epochs
        
        Returns:
            Dict of metrics per category
        """
        print("\nğŸ“ Training per-category classifiers...")
        
        all_metrics = {}
        categories = self.category_manager.ATTACK_CATEGORIES
        
        for category in categories:
            # Create binary labels for this category
            # This requires category-specific labeling from the dataset
            # For now, we'll use a placeholder approach
            
            # In production, you'd filter samples by attack type
            # and train binary classifier for that specific type
            
            print(f"\n   Training {category} classifier...")
            
            # Create synthetic category labels for demo
            # In production, this comes from dataset attack_category labels
            y_train_cat = (np.random.rand(len(X_train)) > 0.8).astype(np.float32)
            y_val_cat = (np.random.rand(len(X_val)) > 0.8).astype(np.float32)
            
            # Balance this category
            X_balanced, y_balanced = self.apply_class_balancing(
                X_train, y_train_cat, strategy='combined'
            )
            
            # Train
            metrics = self.category_manager.train_category(
                category, X_balanced, y_balanced, X_val, y_val_cat, epochs
            )
            all_metrics[category] = metrics
        
        return all_metrics
    
    def train_binary_classifier(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        epochs: int = 50
    ) -> Dict:
        """
        Train main binary (attack vs benign) classifier
        """
        print("\nğŸ“ Training binary classifier...")
        
        # Balance classes
        X_balanced, y_balanced = self.apply_class_balancing(
            X_train, y_train, strategy='combined'
        )
        
        # Create classifier
        self.binary_classifier = create_attack_classifier(
            "binary_detector", self.feature_dim
        )
        
        metrics = self.binary_classifier.train(
            X_balanced, y_balanced, X_val, y_val, epochs
        )
        
        print(f"   âœ… Binary classifier trained - Val Acc: {metrics.get('val_acc', 'N/A')}")
        
        return metrics
    
    def evaluate(
        self,
        X_test: np.ndarray,
        y_test: np.ndarray
    ) -> Dict:
        """
        Comprehensive evaluation on test set
        
        Returns:
            Dict with accuracy, precision, recall, F1, confusion matrix
        """
        print("\nğŸ“Š Evaluating on test set...")
        
        if not SKLEARN_AVAILABLE:
            y_pred = self.binary_classifier.predict(X_test)
            accuracy = np.mean(y_pred == y_test)
            return {'accuracy': accuracy}
        
        # Binary predictions
        y_pred = self.binary_classifier.predict(X_test)
        y_proba = self.binary_classifier.predict_proba(X_test)
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1': f1_score(y_test, y_pred, zero_division=0),
            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()
        }
        
        # AUC if we have probabilities
        try:
            metrics['auc_roc'] = roc_auc_score(y_test, y_proba)
        except:
            pass
        
        # False positive/negative rates
        tn, fp, fn, tp = metrics['confusion_matrix'][0][0], metrics['confusion_matrix'][0][1], \
                         metrics['confusion_matrix'][1][0], metrics['confusion_matrix'][1][1]
        
        metrics['false_positive_rate'] = fp / (fp + tn) if (fp + tn) > 0 else 0
        metrics['false_negative_rate'] = fn / (fn + tp) if (fn + tp) > 0 else 0
        
        print(f"   Accuracy:  {metrics['accuracy']:.4f}")
        print(f"   Precision: {metrics['precision']:.4f}")
        print(f"   Recall:    {metrics['recall']:.4f}")
        print(f"   F1 Score:  {metrics['f1']:.4f}")
        print(f"   FPR:       {metrics['false_positive_rate']:.4f}")
        print(f"   FNR:       {metrics['false_negative_rate']:.4f}")
        
        return metrics
    
    def run_full_pipeline(
        self,
        datasets: List[str] = None,
        epochs: int = 50,
        balance_strategy: str = 'combined'
    ) -> Dict:
        """
        Run complete training pipeline
        
        Args:
            datasets: List of dataset names to use
            epochs: Training epochs
            balance_strategy: Class balancing strategy
        
        Returns:
            Complete training report
        """
        self.training_history['started_at'] = datetime.now().isoformat()
        
        print("=" * 60)
        print("ğŸš€ PCDS ML Training Pipeline")
        print("=" * 60)
        
        # Step 1: Load data
        loaded_data = self.load_data(datasets)
        
        if not loaded_data:
            print("âŒ No datasets loaded. Aborting.")
            return {'error': 'No datasets available'}
        
        # Step 2: Combine datasets
        print("\nğŸ”— Combining datasets...")
        all_X = []
        all_y = []
        
        for name, (X, y_bin, _) in loaded_data.items():
            all_X.append(X)
            all_y.append(y_bin)
        
        X_combined = np.vstack(all_X)
        y_combined = np.concatenate(all_y)
        
        print(f"   Combined: {X_combined.shape[0]} samples, {X_combined.shape[1]} features")
        
        # Step 3: Create splits
        splits = self.create_splits(X_combined, y_combined)
        X_train, y_train = splits['train']
        X_val, y_val = splits['val']
        X_test, y_test = splits['test']
        
        # Step 4: Train binary classifier
        binary_metrics = self.train_binary_classifier(
            X_train, y_train, X_val, y_val, epochs
        )
        
        # Step 5: Train per-category classifiers
        category_metrics = self.train_per_category(
            X_train, y_train, X_val, y_val, epochs
        )
        
        # Step 6: Evaluate
        test_metrics = self.evaluate(X_test, y_test)
        
        # Step 7: Save models
        print("\nğŸ’¾ Saving models...")
        self.save_models()
        
        # Complete training history
        self.training_history['completed_at'] = datetime.now().isoformat()
        self.training_history['metrics'] = {
            'binary': binary_metrics,
            'categories': category_metrics,
            'test': test_metrics
        }
        
        # Save training report
        report_path = self.model_dir / 'training_report.json'
        with open(report_path, 'w') as f:
            json.dump(self.training_history, f, indent=2, default=str)
        
        print("\n" + "=" * 60)
        print("âœ… TRAINING COMPLETE")
        print("=" * 60)
        print(f"   Total samples: {self.training_history['samples_total']}")
        print(f"   Test Accuracy: {test_metrics.get('accuracy', 'N/A'):.4f}")
        print(f"   Test F1 Score: {test_metrics.get('f1', 'N/A'):.4f}")
        print(f"   False Positive Rate: {test_metrics.get('false_positive_rate', 'N/A'):.4f}")
        print(f"   Report saved to: {report_path}")
        
        return self.training_history
    
    def save_models(self):
        """Save all trained models"""
        
        # Save binary classifier
        if hasattr(self, 'binary_classifier') and self.binary_classifier.trained:
            path = self.model_dir / 'binary_detector.pt'
            self.binary_classifier.save(str(path))
        
        # Save category classifiers
        self.category_manager.save_all()
        
        print(f"   âœ… Models saved to {self.model_dir}")
    
    def load_models(self):
        """Load all trained models"""
        
        # Load binary classifier
        path = self.model_dir / 'binary_detector.pt'
        if path.exists():
            self.binary_classifier = create_attack_classifier(
                "binary_detector", self.feature_dim
            )
            self.binary_classifier.load(str(path))
        
        # Load category classifiers
        self.category_manager.load_all()
        
        print(f"   âœ… Models loaded from {self.model_dir}")


# ==================== Convenience Functions ====================

def train_from_dataset(
    dataset_name: str,
    epochs: int = 50,
    model_dir: str = "ml/models/trained"
) -> Dict:
    """
    Quick training from a single dataset
    
    Usage:
        train_from_dataset('cicids2017', epochs=50)
    """
    pipeline = TrainingPipeline(model_dir=model_dir)
    return pipeline.run_full_pipeline(datasets=[dataset_name], epochs=epochs)


def train_all_datasets(epochs: int = 50, model_dir: str = "ml/models/trained") -> Dict:
    """
    Train on all available datasets
    
    Usage:
        train_all_datasets(epochs=100)
    """
    pipeline = TrainingPipeline(model_dir=model_dir)
    return pipeline.run_full_pipeline(epochs=epochs)


if __name__ == "__main__":
    print("ML Training Pipeline - Test Run")
    print("=" * 60)
    
    # Test with synthetic data (since real datasets need to be downloaded)
    print("\nğŸ“Œ Creating synthetic test data...")
    
    # Create synthetic dataset
    np.random.seed(42)
    X_synthetic = np.random.randn(5000, 64).astype(np.float32)
    y_synthetic = (np.random.rand(5000) > 0.7).astype(np.float32)
    
    # Initialize pipeline
    pipeline = TrainingPipeline()
    
    # Create splits
    splits = pipeline.create_splits(X_synthetic, y_synthetic)
    X_train, y_train = splits['train']
    X_val, y_val = splits['val']
    X_test, y_test = splits['test']
    
    # Train binary classifier
    binary_metrics = pipeline.train_binary_classifier(
        X_train, y_train, X_val, y_val, epochs=20
    )
    
    # Evaluate
    test_metrics = pipeline.evaluate(X_test, y_test)
    
    print("\nâœ… Pipeline test complete!")
    print(f"   Test Accuracy: {test_metrics.get('accuracy', 'N/A'):.4f}")
